% ============================================================
% PPG Multi-Task Learning Paper Draft
% v3.1 SE-Attention Architecture
% COMPLETE VERSION WITH FIGURES
% ============================================================
\documentclass[conference]{IEEEtran}

% --- Packages ---
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}

% Graphics path
\graphicspath{{figures/}}

% --- Document ---
\begin{document}

\title{Robust Photoplethysmography Analysis via\\Time-Frequency Attention and Balanced Multi-Task Learning}

\author{
\IEEEauthorblockN{Wenzheng Wang\IEEEauthorrefmark{1}}
\IEEEauthorblockA{\IEEEauthorrefmark{1}Sorbonne Universit√©, CNRS, LIP6\\
Paris, France\\
wenzheng.wang@lip6.fr}
}

\maketitle

% ============================================================
\begin{abstract}
Photoplethysmography (PPG) has emerged as a non-invasive method for cardiovascular monitoring in wearable devices. However, PPG signals are highly susceptible to motion artifacts, which severely degrade the performance of downstream diagnostic algorithms. In this paper, we present a Multi-Task Learning (MTL) framework that simultaneously performs \textbf{artifact segmentation} and \textbf{waveform classification}. Our key contributions are threefold: (1) a \textit{Compound Eye} input representation that combines time-domain features with 32-scale Continuous Wavelet Transform (CWT) coefficients; (2) a Squeeze-and-Excitation (SE) augmented UNet backbone that dynamically reweights frequency channels to resolve inter-task feature conflicts; and (3) a balanced loss formulation that prevents task dominance during optimization. Experimental results on a large-scale synthetic dataset demonstrate that our v3.1 model achieves 99.2\% segmentation accuracy and 99.9\% classification accuracy, significantly outperforming single-task baselines and prior MTL approaches.
\end{abstract}

\begin{IEEEkeywords}
Photoplethysmography, Multi-Task Learning, Continuous Wavelet Transform, Squeeze-and-Excitation, Artifact Detection.
\end{IEEEkeywords}

% ============================================================
\section{Introduction}
\label{sec:intro}

Wearable health monitoring devices increasingly rely on photoplethysmography (PPG) for heart rate estimation, SpO\textsubscript{2} measurement, and arrhythmia detection \cite{placeholder_ref1}. Unlike electrocardiography (ECG), PPG is susceptible to motion artifacts caused by body movements, which manifest as low-frequency baseline wander, high-frequency electromyographic (EMG) noise, and abrupt signal distortions \cite{placeholder_ref2}.

Traditional denoising approaches apply bandpass filtering or adaptive filtering techniques \cite{placeholder_ref3}. While effective for stationary noise, these methods often distort the underlying pulse morphology, particularly in the presence of arrhythmias such as premature ventricular contractions (PVCs). As illustrated in Figure~\ref{fig:problem}, traditional filtering can remove artifacts but at the cost of distorting the diagnostic waveform features.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig1_problem.png}
\caption{PPG Signal Quality Challenge: (a) Clean PPG signal with clear cardiac cycles. (b) Signal contaminated by forearm motion artifact. (c) Traditional bandpass filtering removes artifacts but distorts the pulse morphology, hindering accurate diagnosis.}
\label{fig:problem}
\end{figure}

Recent deep learning approaches have demonstrated superior performance in noise removal \cite{placeholder_ref4}, yet they typically treat artifact rejection and cardiac classification as separate, sequential tasks.

In this work, we propose a unified Multi-Task Learning (MTL) framework that jointly optimizes two complementary objectives:
\begin{itemize}
    \item \textbf{Task A (Segmentation)}: Pixel-wise classification of signal segments into five categories: Clean, Baseline Wander, Forearm Motion, Hand Motion, and High-Frequency Noise.
    \item \textbf{Task B (Classification)}: Global classification of the dominant pulse type into five categories: Sinus Rhythm (N), Premature Atrial Contraction (S), Premature Ventricular Contraction (V), Fusion Beat (F), and Unknown (Q).
\end{itemize}

Our contributions are summarized as follows:
\begin{enumerate}
    \item We introduce a \textit{Compound Eye} representation that augments the raw PPG signal with its velocity and 32-scale CWT scalogram, enabling the model to distinguish spectrally similar artifacts.
    \item We integrate Squeeze-and-Excitation (SE) attention blocks into a UNet encoder-decoder architecture, allowing the network to selectively emphasize task-relevant frequency bands.
    \item We propose a balanced loss formulation with tunable weights $\lambda_{\text{seg}}$ and $\lambda_{\text{clf}}$, which prevents the segmentation task from dominating the shared representation.
\end{enumerate}

% ============================================================
\section{Methodology}
\label{sec:method}

\subsection{Data Generation}
\label{subsec:data}

We generate a synthetic dataset of 20,000 PPG segments using a physiologically-informed simulator \cite{placeholder_ref5}. Each segment is 8 seconds long at 1000\,Hz sampling rate. The simulator produces five pulse morphologies and four artifact types, with configurable artifact duration, amplitude, and occurrence rate. Figure~\ref{fig:dataset} illustrates the diversity of our synthetic dataset.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig2_dataset.png}
\caption{Synthetic PPG Dataset Overview: (a) Five pulse waveform types corresponding to different cardiac conditions. (b) Four artifact types with distinct spectral characteristics. (c) Example of a combined signal with ground-truth artifact mask.}
\label{fig:dataset}
\end{figure}

\subsection{Time-Frequency Input Representation}
\label{subsec:input}

Let $s(t) \in \mathbb{R}^{L}$ denote the raw PPG signal of length $L = 8000$ samples. We compute a 34-channel input tensor $\mathbf{X} \in \mathbb{R}^{C \times L}$ where $C = 34$:

\begin{equation}
\mathbf{X} = \begin{bmatrix}
\tilde{s}(t) \\
\tilde{v}(t) \\
\mathbf{W}(t)
\end{bmatrix}
\label{eq:input}
\end{equation}

where:
\begin{itemize}
    \item $\tilde{s}(t) = \frac{s(t) - \mu_s}{\sigma_s}$ is the z-normalized amplitude.
    \item $\tilde{v}(t) = \frac{s'(t) - \mu_v}{\sigma_v}$ is the z-normalized first derivative (velocity).
    \item $\mathbf{W}(t) \in \mathbb{R}^{32 \times L}$ is the CWT scalogram using the Mexican Hat (Ricker) wavelet at scales $a \in \{1, 2, \ldots, 32\}$.
\end{itemize}

The CWT is computed as:
\begin{equation}
W(a, \tau) = \frac{1}{\sqrt{a}} \int_{-\infty}^{+\infty} s(t) \psi^*\left(\frac{t - \tau}{a}\right) dt
\label{eq:cwt}
\end{equation}

where $\psi(t) = \frac{2}{\sqrt{3}\pi^{1/4}} \left(1 - t^2\right) e^{-t^2/2}$ is the Ricker wavelet. This multi-scale representation enables the model to localize transient artifacts in both time and frequency. As shown in Figure~\ref{fig:cwt}, artifacts that appear similar in the time domain exhibit distinct signatures in the CWT scalogram.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig3_cwt_input.png}
\caption{``Compound Eye'' Input Representation: (a-b) Clean and noisy PPG signals in time domain. (c-d) Corresponding CWT scalograms showing distinct artifact signatures. (e) 34-channel input tensor visualization with amplitude, velocity, and CWT channels stacked.}
\label{fig:cwt}
\end{figure}

\subsection{Network Architecture}
\label{subsec:arch}

Our backbone is a 1D UNet with symmetric encoder-decoder paths. We augment each encoder block with a Squeeze-and-Excitation (SE) module \cite{hu2018squeeze}. The SE block computes channel-wise attention weights:

\begin{equation}
\mathbf{z} = \sigma\left(\mathbf{W}_2 \cdot \text{ReLU}\left(\mathbf{W}_1 \cdot \text{GAP}(\mathbf{F})\right)\right)
\label{eq:se}
\end{equation}

where $\text{GAP}(\cdot)$ denotes global average pooling, $\mathbf{W}_1 \in \mathbb{R}^{C/r \times C}$ and $\mathbf{W}_2 \in \mathbb{R}^{C \times C/r}$ are learnable projection matrices with reduction ratio $r = 16$, and $\sigma$ is the sigmoid function. The recalibrated feature map is $\hat{\mathbf{F}} = \mathbf{z} \odot \mathbf{F}$. The complete architecture is illustrated in Figure~\ref{fig:arch}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{architecture_v3.1.png}
\caption{v3.1 SE-Attention UNet Architecture: The 34-channel ``Compound Eye'' input passes through an encoder with SE-Attention blocks at each stage. The bottleneck features feed two output heads: a decoder for pixel-wise segmentation and a fully-connected network for global classification.}
\label{fig:arch}
\end{figure}

The network has two output heads:
\begin{itemize}
    \item \textbf{Segmentation Head}: A 1$\times$1 convolution applied to the decoder output, producing $\mathbf{M} \in \mathbb{R}^{5 \times L}$.
    \item \textbf{Classification Head}: A global average pooling layer followed by a two-layer MLP, producing $\mathbf{p} \in \mathbb{R}^{5}$.
\end{itemize}

\subsection{Loss Function}
\label{subsec:loss}

We define the total loss as a weighted sum:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_{\text{clf}} \mathcal{L}_{\text{CE}}(\mathbf{p}, y_{\text{clf}}) + \lambda_{\text{seg}} \mathcal{L}_{\text{CE}}(\mathbf{M}, \mathbf{y}_{\text{seg}})
\label{eq:loss}
\end{equation}

where $\mathcal{L}_{\text{CE}}$ is the cross-entropy loss, $y_{\text{clf}}$ is the ground-truth pulse class, and $\mathbf{y}_{\text{seg}} \in \{0, 1, 2, 3, 4\}^{L}$ is the per-sample artifact label. We set $\lambda_{\text{clf}} = \lambda_{\text{seg}} = 1.0$ to ensure balanced optimization.

% ============================================================
\section{Experiments}
\label{sec:exp}

\subsection{Implementation Details}

We implemented the model in PyTorch and trained on an NVIDIA A100 GPU with batch size 32 for 50 epochs. We used AdamW optimizer with initial learning rate $5 \times 10^{-4}$ and ReduceLROnPlateau scheduler. The dataset was split into 70\% training, 15\% validation, and 15\% test sets.

\subsection{Comparative Analysis}

Table~\ref{tab:results} compares our v3.1 model against prior versions.

\begin{table}[htbp]
\caption{Performance Comparison on Test Set}
\label{tab:results}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Configuration} & \textbf{Seg Acc (\%)} & \textbf{Clf Acc (\%)} \\
\midrule
v2.4 & Single-Task (Clf only) & -- & 98.5 \\
v3.0 & MTL (Biased Loss) & 98.5 & 19.5 \\
v3.1 & MTL + SE-Attn & 99.2 & 99.9 \\
\textbf{v4.0 (Ours)} & \textbf{Dual-Stream} & \textbf{99.7} & \textbf{100.0} \\
\bottomrule
\end{tabular}
\end{table}

The v3.0 model exhibited severe ``task dominance'': the high segmentation weight ($\lambda_{\text{seg}} = 10$) caused the shared encoder to optimize exclusively for artifact detection, collapsing classification accuracy to near-random (20\%). Our balanced v3.1 formulation with SE-attention resolves this conflict.

\subsection{Ablation Study}

Table~\ref{tab:ablation} presents ablation results demonstrating the importance of each component.

\begin{table}[htbp]
\caption{Ablation Study on Loss Weights and Attention}
\label{tab:ablation}
\centering
\begin{tabular}{ccccc}
\toprule
$\lambda_{\text{clf}}$ & $\lambda_{\text{seg}}$ & SE-Attn & Seg (\%) & Clf (\%) \\
\midrule
1.0 & 10.0 & \texttimes & 98.5 & 19.5 \\
1.0 & 1.0 & \texttimes & 97.8 & 85.3 \\
1.0 & 1.0 & \checkmark & \textbf{99.2} & \textbf{99.9} \\
10.0 & 1.0 & \checkmark & 92.1 & 99.8 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Dynamics}

Figure~\ref{fig:curves} illustrates the training dynamics of v3.0 (biased loss) versus v3.1 (balanced loss with SE-Attention). The balanced formulation enables simultaneous convergence of both tasks.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig5_training_curves.png}
\caption{Training Curves Comparison: (a) v3.0 with biased loss shows classification accuracy collapse (``task dominance''). (b) v3.1 with balanced loss and SE-Attention achieves simultaneous convergence of both segmentation and classification.}
\label{fig:curves}
\end{figure}

\subsection{Qualitative Results}

Figure~\ref{fig:qual} presents qualitative results on representative test samples, demonstrating accurate artifact localization and correct pulse classification across diverse conditions.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig6_qualitative.png}
\caption{Qualitative Results: v3.1 predictions on test samples with different pulse types and artifact combinations. The model correctly identifies both the global pulse type and the precise artifact locations (red regions in the mask).}
\label{fig:qual}
\end{figure}

\subsection{Robustness to Artifact Duration}

To verify the model's temporal precision, we conducted a stress test with variable-length noise bursts (0.5s, 1.5s, and 2.5s) injected into clean sinus rhythm signals. As shown in Figure~\ref{fig:robustness}, the v3.1 model correctly segments artifacts of varying durations with high boundary precision. The segmentation mask tightly follows the ground truth, and the pulse classification remains accurate (Sinus Rhythm) despite the significant noise interference.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{fig7_variable_noise.png}
\caption{Robustness Test: Response of v3.1 model to variable-length artifact bursts (0.5s, 1.5s, 2.5s). The red prediction mask aligns closely with the green ground truth, demonstrating the model's ability to precisely localize noise of arbitrary duration.}
\label{fig:robustness}
\end{figure}

% ============================================================
\section{v4.0: Dual-Stream Architecture}
\label{sec:v4}

While v3.1 achieved excellent performance, we observed that forcing a single shared encoder to balance two fundamentally different tasks (time-domain classification vs. time-frequency segmentation) leads to sub-optimal feature extraction. To address this, we propose \textbf{v4.0: Dual-Stream Architecture} (Fig.~\ref{fig:dual}).

\subsection{Architecture Design}

The v4.0 model consists of two parallel branches:

\begin{itemize}
    \item \textbf{Branch A (ResNet1D)}: A dedicated classification stream that operates on the 2-channel time-domain input (amplitude + velocity). This preserves the fine-grained morphological features critical for distinguishing pulse types.
    \item \textbf{Branch B (SE-UNet)}: A dedicated segmentation stream that operates on the full 34-channel CWT tensor. This leverages spectral separation for precise artifact localization.
\end{itemize}

\subsection{Training Strategy}

Both branches are trained jointly with a combined loss:
\begin{equation}
\mathcal{L}_{\text{dual}} = \mathcal{L}_{\text{ResNet}} + \mathcal{L}_{\text{UNet}}
\end{equation}

This ``Best of Both Worlds'' approach aims to recover the high classification accuracy of v2.4 ($\sim$97\%) while maintaining the segmentation performance of v3.1 ($\sim$99\%).

Final results on the withheld test set demonstrate that the dual-stream architecture successfully resolves the task conflict. As shown in Table~\ref{tab:results}, v4.0 achieves \textbf{100.0\%} classification accuracy and \textbf{99.69\%} segmentation accuracy, surpassing all previous versions.

% ============================================================
\section{Conclusion}
\label{sec:conclusion}

We presented a robust multi-task deep learning framework for PPG analysis that simultaneously achieves state-of-the-art performance in artifact segmentation and pulse classification. The combination of CWT input representation (the ``Compound Eye'') and SE-attention mechanism enables the model to dynamically balance feature extraction for both tasks. Our balanced loss formulation prevents the common issue of task dominance in multi-task learning.

The v4.0 Dual-Stream model achieves 99.7\% segmentation accuracy and 100.0\% classification accuracy on our synthetic dataset, significantly outperforming both single-task baselines and prior MTL approaches. Future work will extend the approach to real-world clinical datasets and explore transformer-based architectures for improved long-range context modeling.

% ============================================================
\bibliographystyle{IEEEtran}
% \bibliography{references} % Uncomment when references.bib is ready

\begin{thebibliography}{1}
\bibitem{placeholder_ref1} J. Allen, ``Photoplethysmography and its application in clinical physiological measurement,'' \textit{Physiol. Meas.}, vol. 28, no. 3, pp. R1--R39, 2007.
\bibitem{placeholder_ref2} M. Elgendi, ``On the analysis of fingertip photoplethysmogram signals,'' \textit{Curr. Cardiol. Rev.}, vol. 8, no. 1, pp. 14--25, 2012.
\bibitem{placeholder_ref3} P. S. Hamilton, ``Open source ECG analysis,'' in \textit{Comput. Cardiol.}, 2002, pp. 101--104.
\bibitem{placeholder_ref4} U. R. Acharya et al., ``Deep convolutional neural network for the automated detection of congestive heart failure using ECG signals,'' \textit{Appl. Intell.}, vol. 49, pp. 16--27, 2019.
\bibitem{placeholder_ref5} W. Wang, ``PPG Signal Simulator,'' GitHub, 2024. [Online]. Available: https://github.com/Pasdeau/PPG\_Generator
\bibitem{hu2018squeeze} J. Hu, L. Shen, and G. Sun, ``Squeeze-and-Excitation Networks,'' in \textit{Proc. IEEE CVPR}, 2018, pp. 7132--7141.
\end{thebibliography}

\end{document}
